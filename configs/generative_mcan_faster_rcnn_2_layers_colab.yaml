# configuration for paths
path:
  train_json_path: features/annotations/OpenViVQA/openvivqa_train.json
  dev_json_path: features/annotations/OpenViVQA/openvivqa_dev.json
  test_json_path: features/annotations/OpenViVQA/openvivqa_test.json
  image_features_path: features/region_features/OpenViVQA/faster_rcnn
  images_path: None

# configuration for training
training:
  checkpoint_path: saved_models
  learning_rate: 1.
  warmup: 10000
  get_scores: False
  training_beam_size: 5
  evaluating_beam_size: 5

# model configuration
model:
  name: modified_mcan_using_region_faster_rcnn_2_layers
  nhead: 8
  nlayers: 2
  d_model: 512
  d_k: 64
  d_v: 64
  d_ff: 2048
  d_feature: 2048
  dropout: .1
  
  # phobert_base
  # phobert_large
  # bartpho_syllable
  # bartpho_word
  pretrained_language_model_name: None
                                                      
  # PhoBERTModel
  # BARTPhoModel
  # GPT2Model
  pretrained_language_model: None

  language_model_hidden_size: 768

  visual_embedding: standard_visual_embedding
  language_embedding: lstm_language_embedding

  fusion:
    module: standard_fusion_module

    # encoder
    # None
    encoder:
      module: encoder
      args:
        use_aoa: False
      self_attention: 
        
        # scaled_dot_product_attention
        # augmented_geometry_scaled_dot_product_attention
        # augmented_memory_scaled_dot_product_attention
        # None
        module: scaled_dot_product_attention

    guided_encoder:
      module: guided_encoder
      args:
        use_aoa: False

      self_attention: 

        # scaled_dot_product_attention
        # augmented_geometry_scaled_dot_product_attention
        # augmented_memory_scaled_dot_product_attention
        # None
        module: scaled_dot_product_attention 

  decoder:

    # decoder
    # meshed_decoder
    # adaptive_decoder
    # None
    module: decoder
    args:
      use_aoa: False

    self_attention: 
    
      # scaled_dot_product_attention
      # apdative_scaled_dot_product_attention
      # None
      module: scaled_dot_product_attention

    enc_attention: 
      # scaled_dot_product_attention
      # apdative_scaled_dot_product_attention
      # None
      module: scaled_dot_product_attention

# dataset configuration
dataset:
  batch_size: 32
  workers: 2

  # vncorenlp
  # pyvi
  # spacy
  # None
  tokenizer: None

  # fasttext.vi.300d
  # phow2v.syllable.100d
  # phow2v.syllable.300d
  # phow2v.word.100d
  # phow2v.word.300d
  # None
  word_embedding: None
  min_freq: 1