config:
  path:
    train_json_path: features/annotations/OpenViVQA/openvivqa_train.json
    dev_json_path: features/annotations/OpenViVQA/openvivqa_dev.json
    test_json_path: features/annotations/OpenViVQA/openvivqa_test.json
    image_features_path: features/region_features/OpenViVQA/faster_rcnn
    images_path: None

  training:
    checkpoint_path: saved_models
    start_from: None
    learning_rate: 1.
    warmup: 10000
    get_scores: False
    training_beam_size: 5
    evaluating_beam_size: 5

  model:
    name: modified_mcan_using_region
    nhead: 8
    nlayers: 3
    d_model: 512
    d_k: 64
    d_v: 64
    d_ff: 2048
    d_feature: 2048
    dropout: .1
    
    # phobert_base
    # phobert_large
    # bartpho_syllable
    # bartpho_word
    pretrained_language_model_name: None

    # PhoBERTModel
    # BARTPhoModel
    # GPT2Model
    pretrained_language_model: None

    language_model_hidden_size: 768

    transformer:
      args:
        use_img_pos: True

      # encoder
      # None
      encoder:
        module: encoder
        
        # encoder
        # multilevel_encoder
        # None
        layer: encoder
        self_attention: 
          
          # scaled_dot_product_attention
          # augmented_geometry_scaled_dot_product_attention
          # augmented_memory_scaled_dot_product_attention
          # None
          module: scaled_dot_product_attention
          args:
        
        args:
          total_memory: None
          use_aoa: False

      guided_encoder:

        # encoder
        # None
        module: encoder

        # encoder
        # multilevel encoder
        # None
        layer: encoder

        # scaled_dot_product_attention
        # augmented_geometry_scaled_dot_product_attention
        # augmented_memory_scaled_dot_product_attention
        # None
        self_attention: 
          module: scaled_dot_product_attention 
          args:
        
        args:
          total_memory: None
          use_aoa: False

      decoder:

        # decoder
        # meshed_decoder
        # adaptive_decoder
        # None
        module: decoder

        # decoder
        # meshed_decoder
        # adaptive_decoder
        # None
        layer: decoder

        self_attention: 
          # scaled_dot_product_attention
          # apdative_scaled_dot_product_attention
          # None
          module: scaled_dot_product_attention
          args:  

        enc_attention: 
          # scaled_dot_product_attention
          # apdative_scaled_dot_product_attention
          # None
          module: scaled_dot_product_attention
          args:

        args:
          use_aoa: False

  dataset:
    batch_size: 32
    workers: 2

    # vncorenlp
    # pyvi
    # spacy
    # None
    tokenizer: None

    # fasttext.vi.300d
    # phow2v.syllable.100d
    # phow2v.syllable.300d
    # phow2v.word.100d
    # phow2v.word.300d
    # None
    word_embedding: None
    min_freq: 1